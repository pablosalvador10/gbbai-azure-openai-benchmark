{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory C:\\Users\\pablosal\\Desktop\\azure-openai-benchmark-pablosal does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory (change yours)\n",
    "target_directory = r\"C:\\Users\\pablosal\\Desktop\\azure-openai-benchmark-pablosal\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now you can access the environment variables using os.getenv\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "DEPLOYMENT_ID = os.getenv(\"AZURE_AOAI_DEPLOYMENT_NAME\")\n",
    "DEPLOYMENT_VERSION = os.getenv(\"AZURE_AOAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Create a custom logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set the level of this logger. This level acts as a threshold. \n",
    "# Any message logged at this level, or higher, will be passed to this logger's handlers.\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create handlers\n",
    "c_handler = logging.StreamHandler()\n",
    "c_handler.setLevel(logging.DEBUG)\n",
    "c_format = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "c_handler.setFormatter(c_format)\n",
    "logger.addHandler(c_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Sample Runs\n",
    "\n",
    "During a test run, the system periodically outputs statistics every 60 seconds to the standard output (stdout), while logs are directed to the standard error output (stderr). Please note that some metrics may not be immediately visible due to insufficient data.\n",
    "\n",
    "Initiate a load test at a rate of 60 Requests Per Minute (RPM) with an exponential back-off retry strategy. This strategy helps to efficiently handle potential API rate limit issues by gradually increasing the wait time between retries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AZURE_OPENAI_ENDPOINT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     10\u001b[0m log_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs/test/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_str\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.log\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Prepare your command without shell redirection\u001b[39;00m\n\u001b[0;32m     13\u001b[0m command \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenchmark.bench\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--deployment\u001b[39m\u001b[38;5;124m\"\u001b[39m, model,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--duration\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1500\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--context-tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1000\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--max-tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m500\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m---> 21\u001b[0m     \u001b[43mAZURE_OPENAI_ENDPOINT\u001b[49m\n\u001b[0;32m     22\u001b[0m ]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Ensure the logs directory exists\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AZURE_OPENAI_ENDPOINT' is not defined"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "# import subprocess\n",
    "\n",
    "# # Prepare the datetime string for the log filename\n",
    "# now = datetime.datetime.now()\n",
    "# now_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# model = \"gpt-4-0125-paygo\"\n",
    "# region = \"southcentralus\"\n",
    "# date_str, time_str = now_str.split('_')\n",
    "# log_file_path = f\"logs/test/{model}/{region}/{date_str}/{time_str.replace('_', ' ')}.log\"\n",
    "\n",
    "# # Prepare your command without shell redirection\n",
    "# command = [\n",
    "#     \"python\", \"-m\", \"benchmark.bench\", \"load\",\n",
    "#     \"--deployment\", model,\n",
    "#     \"--rate\", \"50\",\n",
    "#     \"--duration\", \"1500\",\n",
    "#     \"--shape\", \"custom\",\n",
    "#     \"--context-tokens\", \"1000\",\n",
    "#     \"--max-tokens\", \"500\",\n",
    "#     AZURE_OPENAI_ENDPOINT\n",
    "# ]\n",
    "\n",
    "# # Ensure the logs directory exists\n",
    "# import os\n",
    "# os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "\n",
    "# # Open the log file in write mode\n",
    "# with open(log_file_path, 'w') as log_file:\n",
    "#     # Execute the command and redirect both stdout and stderr to the log file\n",
    "#     process = subprocess.run(command, stdout=log_file, stderr=subprocess.STDOUT, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.client import BenchmarkingTool\n",
    "\n",
    "# Create a client\n",
    "benchmarking_client = BenchmarkingTool(\n",
    "    model=\"gpt-4-0125-ptu\",\n",
    "    region=\"southcentralus\",\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 12:42:04,412 - micro - MainProcess - INFO     Initiating load generation tests. Log output will be directed to: logs/gpt-4-0125-ptu/southcentralus/2024-03-10/12-42-04.log (client.py:run_tests:195)\n",
      "2024-03-10 12:42:04,412 - micro - MainProcess - INFO     Executing command: python -m benchmark.bench load --api-version 2023-05-15 --api-key-env OPENAI_API_KEY --clients 20 --duration 120 --run-end-condition-mode or --rate 50 --aggregation-window 60 --context-generation-method generate --shape-profile custom --context-tokens 1000 --max-tokens 500 --prevent-server-caching True --completions 1 --output-format human --log-save-dir logs/ --retry none --deployment gpt-4-0125-ptu https://gbb-ea-openai-southcentralus-01.openai.azure.com/ (client.py:run_tests:196)\n",
      "2024-03-10 12:44:29,603 - micro - MainProcess - INFO     Load generation tests have completed. Please refer to logs/gpt-4-0125-ptu/southcentralus/2024-03-10/12-42-04.log for the detailed logs. (client.py:run_tests:204)\n"
     ]
    }
   ],
   "source": [
    "benchmarking_client.run_tests(deployment=\"gpt-4-0125-ptu\",\n",
    "                              rate=50,\n",
    "                              duration=120,\n",
    "                              shape_profile=\"custom\",\n",
    "                              context_tokens=1000,\n",
    "                              max_tokens=500,\n",
    "                              log_save_dir=\"logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# import subprocess\n",
    "\n",
    "# # Prepare the datetime string for the log filename\n",
    "# now = datetime.datetime.now()\n",
    "# now_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# log_file_path = f\"logs/test/benchmark_{now_str}.log\"\n",
    "\n",
    "# # Prepare your command without shell redirection\n",
    "# command = [\n",
    "#     \"python\", \"-m\", \"benchmark.bench\", \"load\",\n",
    "#     \"--deployment\", DEPLOYMENT_ID,\n",
    "#     \"--rate\", \"50\",\n",
    "#     \"--shape-profile\", \"custom\",\n",
    "#     \"--duration\", \"60\",\n",
    "#     \"--clients\", \"1\",\n",
    "#     \"--context-tokens\", \"1000\",\n",
    "#     \"--max-tokens\", \"200\",\n",
    "#     \"--completions\", \"200\",\n",
    "#     \"--retry\", \"exponential\",\n",
    "#     AZURE_OPENAI_ENDPOINT\n",
    "# ]\n",
    "\n",
    "# # Ensure the logs directory exists\n",
    "# import os\n",
    "# os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "\n",
    "# # Open the log file in write mode\n",
    "# with open(log_file_path, 'w') as log_file:\n",
    "#     # Execute the command and redirect both stdout and stderr to the log file\n",
    "#     process = subprocess.run(command, stdout=log_file, stderr=subprocess.STDOUT, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     process.kill()\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred while trying to kill the process: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptu-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
