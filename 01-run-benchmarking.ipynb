{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Prerequisites\n",
    "\n",
    "Ensure that your Azure Services are properly set up, your Conda environment is created, and your environment variables are configured as per the instructions in the [SETTINGS.md](SETTINGS.md) file.\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "\n",
    "This notebook assists in conducting a comprehensive performance assessment for Azure OpenAI endpoints, focusing on the operational efficiency of the model in processing requests. The following sections are covered:\n",
    "\n",
    "1. [**Latency Testing**](#latency-testing): This section explores how to conduct latency tests on Azure OpenAI endpoints. Latency measures the response time for a request, assessing how quickly the model responds to a specific request.\n",
    "\n",
    "2. [**Throughput Testing**](#throughput-testing): This part details the steps to perform throughput tests on Azure OpenAI endpoints. Throughput evaluates the number of requests the model can handle in a given time frame, providing an understanding of the model's capacity and efficiency.\n",
    "\n",
    "3. [**Analyzing Test Results**](#analyzing-test-results): This section provides guidance on how to analyze the results from the latency and throughput tests, helping you understand the performance metrics and their implications on the operational efficiency of your model.\n",
    "\n",
    "For additional information, refer to the following resources:\n",
    "- [Azure OpenAI API Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory C:\\Users\\pablosal\\Desktop\\azure-openai-benchmark does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the target directory (change yours)\n",
    "TARGET_DIRECTORY = r\"C:\\Users\\pablosal\\Desktop\\azure-openai-benchmark\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(TARGET_DIRECTORY):\n",
    "    # Change the current working directory\n",
    "    os.chdir(TARGET_DIRECTORY)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {TARGET_DIRECTORY} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP \n",
    "MODEL = \"gpt-4-turbo-2024-04-09-ptu\"\n",
    "REGION = \"swedencentral\"\n",
    "OAZURE_OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY_SWEEDENCENTRAL_PTU\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT_SWEEDENCENTRAL_PTU\")\n",
    "AZURE_OPENAI_API_VERSION = \"2024-02-15-preview\"\n",
    "\n",
    "filename = f\"benchmarks/gpt-4-turbo/{REGION}/{MODEL}/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Latency Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.performance.aoaihelpers.latencytest import (\n",
    "    AzureOpenAIBenchmarkStreaming,\n",
    "    AzureOpenAIBenchmarkNonStreaming,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the deployment names and tokens\n",
    "deployment_names = [MODEL]\n",
    "max_tokens_list = [100, 600, 700, 800, 900, 1000]\n",
    "num_iterations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 18:08:27,141 - micro - MainProcess - INFO     CPU usage: 18.9% (utils.py:log_system_info:200)\n",
      "2024-05-13 18:08:27,151 - micro - MainProcess - INFO     RAM usage: 87.1% (utils.py:log_system_info:202)\n",
      "2024-05-13 18:08:27,624 - micro - MainProcess - INFO     Initiating call for Model: gpt-4-turbo-2024-04-09-ptu, Max Tokens: 100 (latencytest.py:make_call:514)\n",
      "INFO:micro:Initiating call for Model: gpt-4-turbo-2024-04-09-ptu, Max Tokens: 100\n",
      "2024-05-13 18:08:27,656 - micro - MainProcess - INFO     CPU usage: 12.8% (utils.py:log_system_info:200)\n",
      "INFO:micro:CPU usage: 12.8%\n",
      "2024-05-13 18:08:27,671 - micro - MainProcess - INFO     RAM usage: 87.6% (utils.py:log_system_info:202)\n",
      "INFO:micro:RAM usage: 87.6%\n",
      "2024-05-13 18:08:27,785 - micro - MainProcess - INFO     Initiating call for Model: gpt-4-turbo-2024-04-09-ptu, Max Tokens: 600 (latencytest.py:make_call:514)\n",
      "INFO:micro:Initiating call for Model: gpt-4-turbo-2024-04-09-ptu, Max Tokens: 600\n",
      "2024-05-13 18:08:27,788 - micro - MainProcess - INFO     CPU usage: 5.9% (utils.py:log_system_info:200)\n",
      "INFO:micro:CPU usage: 5.9%\n",
      "2024-05-13 18:08:27,802 - micro - MainProcess - INFO     RAM usage: 87.7% (utils.py:log_system_info:202)\n",
      "INFO:micro:RAM usage: 87.7%\n",
      "2024-05-13 18:08:27,930 - micro - MainProcess - INFO     Initiating call for Model: gpt-4-turbo-2024-04-09-ptu, Max Tokens: 700 (latencytest.py:make_call:514)\n",
      "INFO:micro:Initiating call for Model: gpt-4-turbo-2024-04-09-ptu, Max Tokens: 700\n",
      "2024-05-13 18:08:27,934 - micro - MainProcess - INFO     CPU usage: 19.0% (utils.py:log_system_info:200)\n",
      "INFO:micro:CPU usage: 19.0%\n",
      "2024-05-13 18:08:27,948 - micro - MainProcess - INFO     RAM usage: 87.9% (utils.py:log_system_info:202)\n",
      "INFO:micro:RAM usage: 87.9%\n",
      "2024-05-13 18:08:28,101 - micro - MainProcess - INFO     Initiating call for Model: gpt-4-turbo-2024-04-09-ptu, Max Tokens: 800 (latencytest.py:make_call:514)\n",
      "INFO:micro:Initiating call for Model: gpt-4-turbo-2024-04-09-ptu, Max Tokens: 800\n",
      "2024-05-13 18:08:28,107 - micro - MainProcess - INFO     CPU usage: 47.8% (utils.py:log_system_info:200)\n",
      "INFO:micro:CPU usage: 47.8%\n",
      "2024-05-13 18:08:28,124 - micro - MainProcess - INFO     RAM usage: 88.1% (utils.py:log_system_info:202)\n",
      "INFO:micro:RAM usage: 88.1%\n",
      "2024-05-13 18:08:28,265 - micro - MainProcess - INFO     Initiating call for Model: gpt-4-turbo-2024-04-09-ptu, Max Tokens: 900 (latencytest.py:make_call:514)\n",
      "INFO:micro:Initiating call for Model: gpt-4-turbo-2024-04-09-ptu, Max Tokens: 900\n",
      "2024-05-13 18:08:28,269 - micro - MainProcess - INFO     CPU usage: 24.6% (utils.py:log_system_info:200)\n",
      "INFO:micro:CPU usage: 24.6%\n",
      "2024-05-13 18:08:28,285 - micro - MainProcess - INFO     RAM usage: 88.1% (utils.py:log_system_info:202)\n",
      "INFO:micro:RAM usage: 88.1%\n",
      "2024-05-13 18:08:28,402 - micro - MainProcess - INFO     Initiating call for Model: gpt-4-turbo-2024-04-09-ptu, Max Tokens: 1000 (latencytest.py:make_call:514)\n",
      "INFO:micro:Initiating call for Model: gpt-4-turbo-2024-04-09-ptu, Max Tokens: 1000\n",
      "2024-05-13 18:08:33,047 - micro - MainProcess - WARNING  x-ratelimit-remaining-tokens is None in headers (utils.py:extract_rate_limit_and_usage_info_async:67)\n",
      "WARNING:micro:x-ratelimit-remaining-tokens is None in headers\n",
      "2024-05-13 18:08:39,662 - micro - MainProcess - INFO     Succesful Run - Time taken: 5.42 seconds. (latencytest.py:make_call:536)\n",
      "INFO:micro:Succesful Run - Time taken: 5.42 seconds.\n",
      "2024-05-13 18:08:46,752 - micro - MainProcess - WARNING  x-ratelimit-remaining-tokens is None in headers (utils.py:extract_rate_limit_and_usage_info_async:67)\n",
      "WARNING:micro:x-ratelimit-remaining-tokens is None in headers\n",
      "2024-05-13 18:08:46,755 - micro - MainProcess - INFO     Succesful Run - Time taken: 18.55 seconds. (latencytest.py:make_call:536)\n",
      "INFO:micro:Succesful Run - Time taken: 18.55 seconds.\n",
      "2024-05-13 18:08:47,691 - micro - MainProcess - WARNING  x-ratelimit-remaining-tokens is None in headers (utils.py:extract_rate_limit_and_usage_info_async:67)\n",
      "WARNING:micro:x-ratelimit-remaining-tokens is None in headers\n",
      "2024-05-13 18:08:47,695 - micro - MainProcess - INFO     Succesful Run - Time taken: 19.78 seconds. (latencytest.py:make_call:536)\n",
      "INFO:micro:Succesful Run - Time taken: 19.78 seconds.\n",
      "2024-05-13 18:08:49,790 - micro - MainProcess - WARNING  x-ratelimit-remaining-tokens is None in headers (utils.py:extract_rate_limit_and_usage_info_async:67)\n",
      "WARNING:micro:x-ratelimit-remaining-tokens is None in headers\n",
      "2024-05-13 18:08:49,792 - micro - MainProcess - INFO     Succesful Run - Time taken: 21.25 seconds. (latencytest.py:make_call:536)\n",
      "INFO:micro:Succesful Run - Time taken: 21.25 seconds.\n",
      "2024-05-13 18:08:51,672 - micro - MainProcess - WARNING  x-ratelimit-remaining-tokens is None in headers (utils.py:extract_rate_limit_and_usage_info_async:67)\n",
      "WARNING:micro:x-ratelimit-remaining-tokens is None in headers\n",
      "2024-05-13 18:08:51,676 - micro - MainProcess - INFO     Succesful Run - Time taken: 23.44 seconds. (latencytest.py:make_call:536)\n",
      "INFO:micro:Succesful Run - Time taken: 23.44 seconds.\n",
      "2024-05-13 18:08:53,592 - micro - MainProcess - WARNING  x-ratelimit-remaining-tokens is None in headers (utils.py:extract_rate_limit_and_usage_info_async:67)\n",
      "WARNING:micro:x-ratelimit-remaining-tokens is None in headers\n",
      "2024-05-13 18:08:53,596 - micro - MainProcess - INFO     Succesful Run - Time taken: 25.20 seconds. (latencytest.py:make_call:536)\n",
      "INFO:micro:Succesful Run - Time taken: 25.20 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the benchmarking class\n",
    "client_non_streaming = AzureOpenAIBenchmarkNonStreaming(\n",
    "    api_key=OAZURE_OPENAI_API_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT, api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "# Run the benchmark tests\n",
    "await client_non_streaming.run_latency_benchmark_bulk(\n",
    "    deployment_names, max_tokens_list, iterations=num_iterations, context_tokens=1000, multiregion=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------------+----------------+--------------------+----------+----------------------+----------------------+---------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|         Model_MaxTokens         | Iterations |    Regions     |    Median Time     | IQR Time | 95th Percentile Time | 99th Percentile Time | CV Time | Median Prompt Tokens | IQR Prompt Tokens | Median Completion Tokens | IQR Completion Tokens | 95th Percentile Completion Tokens | 99th Percentile Completion Tokens | CV Completion Tokens | Error Rate | Error Types | Successful Runs | Unsuccessful Runs | Throttle Count |                                                                                  Best Run                                                                                   |                                                                                  Worst Run                                                                                  |\n",
      "+---------------------------------+------------+----------------+--------------------+----------+----------------------+----------------------+---------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| gpt-4-turbo-2024-04-09-ptu_100  |     1      | Sweden Central | 5.4190826416015625 |   0.0    |  5.4190826416015625  |  5.4190826416015625  |   0.0   |        1005.0        |        0.0        |          100.0           |          0.0          |               100.0               |               100.0               |         0.0          |    0.0     |     []      |        1        |         0         |       0        | {\"time\": 5.4190826416015625, \"completion_tokens\": 100, \"prompt_tokens\": 1005, \"region\": \"Sweden Central\", \"utilization\": \"6.67%\", \"local_time\": \"2024-05-14 01:08:39 CEST\"} | {\"time\": 5.4190826416015625, \"completion_tokens\": 100, \"prompt_tokens\": 1005, \"region\": \"Sweden Central\", \"utilization\": \"6.67%\", \"local_time\": \"2024-05-14 01:08:39 CEST\"} |\n",
      "| gpt-4-turbo-2024-04-09-ptu_700  |     1      | Sweden Central | 18.548062562942505 |   0.0    |  18.548062562942505  |  18.548062562942505  |   0.0   |        1008.0        |        0.0        |          700.0           |          0.0          |               700.0               |               700.0               |         0.0          |    0.0     |     []      |        1        |         0         |       0        | {\"time\": 18.548062562942505, \"completion_tokens\": 700, \"prompt_tokens\": 1008, \"region\": \"Sweden Central\", \"utilization\": \"3.33%\", \"local_time\": \"2024-05-14 01:08:46 CEST\"} | {\"time\": 18.548062562942505, \"completion_tokens\": 700, \"prompt_tokens\": 1008, \"region\": \"Sweden Central\", \"utilization\": \"3.33%\", \"local_time\": \"2024-05-14 01:08:46 CEST\"} |\n",
      "| gpt-4-turbo-2024-04-09-ptu_600  |     1      | Sweden Central | 19.776206493377686 |   0.0    |  19.776206493377686  |  19.776206493377686  |   0.0   |        1004.0        |        0.0        |          600.0           |          0.0          |               600.0               |               600.0               |         0.0          |    0.0     |     []      |        1        |         0         |       0        | {\"time\": 19.776206493377686, \"completion_tokens\": 600, \"prompt_tokens\": 1004, \"region\": \"Sweden Central\", \"utilization\": \"0.00%\", \"local_time\": \"2024-05-14 01:08:47 CEST\"} | {\"time\": 19.776206493377686, \"completion_tokens\": 600, \"prompt_tokens\": 1004, \"region\": \"Sweden Central\", \"utilization\": \"0.00%\", \"local_time\": \"2024-05-14 01:08:47 CEST\"} |\n",
      "| gpt-4-turbo-2024-04-09-ptu_1000 |     1      | Sweden Central | 21.248290300369263 |   0.0    |  21.248290300369263  |  21.248290300369263  |   0.0   |        1005.0        |        0.0        |          829.0           |          0.0          |               829.0               |               829.0               |         0.0          |    0.0     |     []      |        1        |         0         |       0        | {\"time\": 21.248290300369263, \"completion_tokens\": 829, \"prompt_tokens\": 1005, \"region\": \"Sweden Central\", \"utilization\": \"1.66%\", \"local_time\": \"2024-05-14 01:08:49 CEST\"} | {\"time\": 21.248290300369263, \"completion_tokens\": 829, \"prompt_tokens\": 1005, \"region\": \"Sweden Central\", \"utilization\": \"1.66%\", \"local_time\": \"2024-05-14 01:08:49 CEST\"} |\n",
      "| gpt-4-turbo-2024-04-09-ptu_800  |     1      | Sweden Central | 23.43543577194214  |   0.0    |  23.43543577194214   |  23.43543577194214   |   0.0   |        1007.0        |        0.0        |          767.0           |          0.0          |               767.0               |               767.0               |         0.0          |    0.0     |     []      |        1        |         0         |       0        | {\"time\": 23.43543577194214, \"completion_tokens\": 767, \"prompt_tokens\": 1007, \"region\": \"Sweden Central\", \"utilization\": \"5.00%\", \"local_time\": \"2024-05-14 01:08:51 CEST\"}  | {\"time\": 23.43543577194214, \"completion_tokens\": 767, \"prompt_tokens\": 1007, \"region\": \"Sweden Central\", \"utilization\": \"5.00%\", \"local_time\": \"2024-05-14 01:08:51 CEST\"}  |\n",
      "| gpt-4-turbo-2024-04-09-ptu_900  |     1      | Sweden Central | 25.196763515472412 |   0.0    |  25.196763515472412  |  25.196763515472412  |   0.0   |        1006.0        |        0.0        |          847.0           |          0.0          |               847.0               |               847.0               |         0.0          |    0.0     |     []      |        1        |         0         |       0        | {\"time\": 25.196763515472412, \"completion_tokens\": 847, \"prompt_tokens\": 1006, \"region\": \"Sweden Central\", \"utilization\": \"8.14%\", \"local_time\": \"2024-05-14 01:08:53 CEST\"} | {\"time\": 25.196763515472412, \"completion_tokens\": 847, \"prompt_tokens\": 1006, \"region\": \"Sweden Central\", \"utilization\": \"8.14%\", \"local_time\": \"2024-05-14 01:08:53 CEST\"} |\n",
      "+---------------------------------+------------+----------------+--------------------+----------+----------------------+----------------------+---------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "stats = client_non_streaming.calculate_and_show_statistics()\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "latency_file_name = filename + f\"results_iterations={num_iterations}_time={timestamp}.json\"\n",
    "client_non_streaming.save_statistics_to_file(stats, location=latency_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now you can access the environment variables using os.getenv\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "DEPLOYMENT_ID = os.getenv(\"AZURE_AOAI_DEPLOYMENT_NAME\")\n",
    "DEPLOYMENT_VERSION = os.getenv(\"AZURE_AOAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Create a custom logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set the level of this logger. This level acts as a threshold.\n",
    "# Any message logged at this level, or higher, will be passed to this logger's handlers.\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create handlers\n",
    "c_handler = logging.StreamHandler()\n",
    "c_handler.setLevel(logging.DEBUG)\n",
    "c_format = logging.Formatter(\"%(name)s - %(levelname)s - %(message)s\")\n",
    "c_handler.setFormatter(c_format)\n",
    "logger.addHandler(c_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.performance.client import LoadTestBenchmarking\n",
    "\n",
    "# Create a client\n",
    "benchmarking_client = LoadTestBenchmarking(\n",
    "    model=\"gpt-4-1106-pagyo\",\n",
    "    region=\"swedencentral\",\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarking_client.run_tests(\n",
    "    deployment=\"gpt-4-1106-pagyo\",\n",
    "    rate=5,\n",
    "    duration=180,\n",
    "    shape_profile=\"custom\",\n",
    "    clients=10,\n",
    "    context_tokens=1000,\n",
    "    max_tokens=500,\n",
    "    prevent_server_caching=True,\n",
    "    log_save_dir=\"logs/\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptu-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
