{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbbai-azure-openai-benchmark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the target directory (change yours)\n",
    "TARGET_DIRECTORY = r\"C:\\Users\\pablosal\\Desktop\\gbbai-azure-openai-benchmark\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(TARGET_DIRECTORY):\n",
    "    # Change the current working directory\n",
    "    os.chdir(TARGET_DIRECTORY)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {TARGET_DIRECTORY} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP \n",
    "MODEL = \"gpt-4-turbo-2024-04-09-payg\"\n",
    "REGION = \"eastus2\"\n",
    "OAZURE_OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY_EAST2\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT_EAST2\")\n",
    "AZURE_OPENAI_API_VERSION = \"2024-02-15-preview\"\n",
    "\n",
    "filename = f\"benchmarks/gpt-4-turbo/{REGION}/{MODEL}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.performance.latencytest import (\n",
    "    AzureOpenAIBenchmarkNonStreaming,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the deployment names and tokens\n",
    "deployment_names = [MODEL]\n",
    "max_tokens_list = [100]\n",
    "num_iterations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 14:45:52,381 - micro - MainProcess - INFO     CPU usage: 11.9% (utils.py:log_system_info:214)\n",
      "2024-05-27 14:45:52,400 - micro - MainProcess - INFO     RAM usage: 67.9% (utils.py:log_system_info:216)\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002810C5AF9A0>: Failed to resolve 'openaipublic.blob.core.windows.net' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\socket.py:954\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    953\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 954\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    955\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\urllib3\\connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\urllib3\\connectionpool.py:1096\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1096\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\urllib3\\connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    610\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 611\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    612\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\urllib3\\connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000002810C5AF9A0>: Failed to resolve 'openaipublic.blob.core.windows.net' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\urllib3\\connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    842\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 844\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    847\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002810C5AF9A0>: Failed to resolve 'openaipublic.blob.core.windows.net' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m client_non_streaming \u001b[38;5;241m=\u001b[39m AzureOpenAIBenchmarkNonStreaming(\n\u001b[0;32m      3\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mOAZURE_OPENAI_API_KEY, azure_endpoint\u001b[38;5;241m=\u001b[39mAZURE_OPENAI_ENDPOINT, api_version\u001b[38;5;241m=\u001b[39mAZURE_OPENAI_API_VERSION\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Run the benchmark tests\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m client_non_streaming\u001b[38;5;241m.\u001b[39mrun_latency_benchmark_bulk(\n\u001b[0;32m      8\u001b[0m     deployment_names, max_tokens_list, iterations\u001b[38;5;241m=\u001b[39mnum_iterations, context_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, multiregion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\Desktop\\gbbai-azure-openai-benchmark\\src\\performance\\latencytest.py:183\u001b[0m, in \u001b[0;36mAzureOpenAIBenchmarkLatency.run_latency_benchmark_bulk\u001b[1;34m(self, deployment_names, max_tokens_list, same_model_interval, different_model_interval, iterations, temperature, context_tokens, multiregion, prevent_server_caching)\u001b[0m\n\u001b[0;32m    166\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_latency_benchmark(\n\u001b[0;32m    168\u001b[0m         [deployment_name],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m max_tokens \u001b[38;5;129;01min\u001b[39;00m max_tokens_list\n\u001b[0;32m    180\u001b[0m ]\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Run tasks concurrently\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\Desktop\\gbbai-azure-openai-benchmark\\src\\performance\\latencytest.py:129\u001b[0m, in \u001b[0;36mAzureOpenAIBenchmarkLatency.run_latency_benchmark\u001b[1;34m(self, deployment_names, max_tokens_list, iterations, same_model_interval, different_model_interval, temperature, context_tokens, multiregion, prevent_server_caching)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[0;32m    128\u001b[0m         log_system_info()\n\u001b[1;32m--> 129\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_call(\n\u001b[0;32m    130\u001b[0m             deployment_name,\n\u001b[0;32m    131\u001b[0m             max_tokens,\n\u001b[0;32m    132\u001b[0m             temperature,\n\u001b[0;32m    133\u001b[0m             context_tokens,\n\u001b[0;32m    134\u001b[0m             prevent_server_caching\n\u001b[0;32m    135\u001b[0m         )\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(same_model_interval)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(different_model_interval)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\Desktop\\gbbai-azure-openai-benchmark\\src\\performance\\latencytest.py:506\u001b[0m, in \u001b[0;36mAzureOpenAIBenchmarkNonStreaming.make_call\u001b[1;34m(self, deployment_name, max_tokens, temperature, context_tokens, prevent_server_caching)\u001b[0m\n\u001b[0;32m    501\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAs no context was provided, 1000 tokens were added as average workloads.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     context_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m--> 506\u001b[0m random \u001b[38;5;241m=\u001b[39m \u001b[43mRandomMessagesGenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprevent_server_caching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprevent_server_caching\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m messages, _ \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mgenerate_messages()\n\u001b[0;32m    514\u001b[0m body \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    522\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\Desktop\\gbbai-azure-openai-benchmark\\src\\performance\\messagegeneration.py:105\u001b[0m, in \u001b[0;36mRandomMessagesGenerator.__init__\u001b[1;34m(self, model, prevent_server_caching, tokens, max_tokens)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    100\u001b[0m         {\n\u001b[0;32m    101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    102\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite a long essay about machine learning in at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    103\u001b[0m         }\n\u001b[0;32m    104\u001b[0m     )\n\u001b[1;32m--> 105\u001b[0m messages_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnum_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevent_server_caching:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# Add anticache prefix before we start generating random words to ensure\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# token count when used in testing is correct\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     messages, messages_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_anticache_prefix(\n\u001b[0;32m    110\u001b[0m         messages, messages_tokens\n\u001b[0;32m    111\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\Desktop\\gbbai-azure-openai-benchmark\\src\\performance\\oaitokenizer.py:21\u001b[0m, in \u001b[0;36mnum_tokens_from_messages\u001b[1;34m(messages, model)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_tokens_from_messages\u001b[39m(messages, model):\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the number of tokens used by a list of messages.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[43mtiktoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-0613\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-16k-0613\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4-32k-0613\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m     }:\n\u001b[0;32m     31\u001b[0m         tokens_per_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\tiktoken\\model.py:97\u001b[0m, in \u001b[0;36mencoding_for_model\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoding_for_model\u001b[39m(model_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Encoding:\n\u001b[0;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the encoding used by a model.\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    Raises a KeyError if the model name is not recognised.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding_name_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\tiktoken\\registry.py:73\u001b[0m, in \u001b[0;36mget_encoding\u001b[1;34m(encoding_name)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown encoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoding_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Plugins found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_available_plugin_modules()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[0;32m     72\u001b[0m constructor \u001b[38;5;241m=\u001b[39m ENCODING_CONSTRUCTORS[encoding_name]\n\u001b[1;32m---> 73\u001b[0m enc \u001b[38;5;241m=\u001b[39m Encoding(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mconstructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     74\u001b[0m ENCODINGS[encoding_name] \u001b[38;5;241m=\u001b[39m enc\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m enc\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\tiktoken_ext\\openai_public.py:64\u001b[0m, in \u001b[0;36mcl100k_base\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcl100k_base\u001b[39m():\n\u001b[1;32m---> 64\u001b[0m     mergeable_ranks \u001b[38;5;241m=\u001b[39m \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     special_tokens \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     68\u001b[0m         ENDOFTEXT: \u001b[38;5;241m100257\u001b[39m,\n\u001b[0;32m     69\u001b[0m         FIM_PREFIX: \u001b[38;5;241m100258\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m         ENDOFPROMPT: \u001b[38;5;241m100276\u001b[39m,\n\u001b[0;32m     73\u001b[0m     }\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl100k_base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpat_str\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m(?i:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md)|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m]?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,3}| ?[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m]+[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn]*|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn]+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+(?!\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS)|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmergeable_ranks\u001b[39m\u001b[38;5;124m\"\u001b[39m: mergeable_ranks,\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecial_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: special_tokens,\n\u001b[0;32m     79\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\tiktoken\\load.py:123\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[1;34m(tiktoken_bpe_file)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     contents \u001b[38;5;241m=\u001b[39m \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    125\u001b[0m         base64\u001b[38;5;241m.\u001b[39mb64decode(token): \u001b[38;5;28mint\u001b[39m(rank)\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token, rank \u001b[38;5;129;01min\u001b[39;00m (line\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m contents\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;28;01mif\u001b[39;00m line)\n\u001b[0;32m    127\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\tiktoken\\load.py:50\u001b[0m, in \u001b[0;36mread_file_cached\u001b[1;34m(blobpath)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cache_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 50\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblobpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(cache_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\tiktoken\\load.py:24\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(blobpath)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# avoiding blobfile for public files helps avoid auth issues, like MFA prompts\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblobpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m resp\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002810C5AF9A0>: Failed to resolve 'openaipublic.blob.core.windows.net' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "# Create an instance of the benchmarking class\n",
    "client_non_streaming = AzureOpenAIBenchmarkNonStreaming(\n",
    "    api_key=OAZURE_OPENAI_API_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT, api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "# Run the benchmark tests\n",
    "await client_non_streaming.run_latency_benchmark_bulk(\n",
    "    deployment_names, max_tokens_list, iterations=num_iterations, context_tokens=1000, multiregion=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[33m                                                                                            [100%]\u001b[0m\n",
      "\u001b[33m======================================== warnings summary =========================================\u001b[0m\n",
      "..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\_pytest\\config\\__init__.py:1285\n",
      "  c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\_pytest\\config\\__init__.py:1285: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m\u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 0.05s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "from src.performance.latencytest import AzureOpenAIBenchmarkNonStreaming\n",
    "from unittest.mock import patch, MagicMock\n",
    "\n",
    "def test_validate_api_configuration():\n",
    "    with patch.object(AzureOpenAIBenchmarkNonStreaming, '__init__', return_value=None) as mock_init:\n",
    "\n",
    "        AzureOpenAIBenchmarkNonStreaming.azure_endpoint = 'mocked_endpoint' \n",
    "        AzureOpenAIBenchmarkNonStreaming.api_key = \"mocked_key\"\n",
    "        AzureOpenAIBenchmarkNonStreaming.api_version = \"mocked_version\"\n",
    "        \n",
    "        az = AzureOpenAIBenchmarkNonStreaming()\n",
    "        az._validate_api_configurations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:43:22,961 - micro - MainProcess - INFO     Calculating statistics for data: [] (utils.py:calculate_statistics:158)\n",
      "2024-05-27 13:43:22,963 - micro - MainProcess - INFO     No data provided. Returning result: (None, None, None, None, None) (utils.py:calculate_statistics:162)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:43:22,969 - micro - MainProcess - INFO     Calculating statistics for data: [2, 2, 2, 2, 2] (utils.py:calculate_statistics:158)\n",
      "2024-05-27 13:43:22,969 - micro - MainProcess - INFO     Data converted to numpy array: [2 2 2 2 2] (utils.py:calculate_statistics:166)\n",
      "2024-05-27 13:43:22,977 - micro - MainProcess - INFO     Calculated median: 2.0 (utils.py:calculate_statistics:170)\n",
      "2024-05-27 13:43:22,986 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:174)\n",
      "2024-05-27 13:43:22,992 - micro - MainProcess - INFO     Calculated 95th percentile: 2.0 (utils.py:calculate_statistics:178)\n",
      "2024-05-27 13:43:22,995 - micro - MainProcess - INFO     Calculated 99th percentile: 2.0 (utils.py:calculate_statistics:182)\n",
      "2024-05-27 13:43:23,001 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:186)\n",
      "2024-05-27 13:43:23,004 - micro - MainProcess - INFO     Result: (2.0, 0.0, 2.0, 2.0, 0.0) (utils.py:calculate_statistics:189)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:43:23,014 - micro - MainProcess - INFO     Calculating statistics for data: [2, 4, 6, 8, 10, 12, 56, 34] (utils.py:calculate_statistics:158)\n",
      "2024-05-27 13:43:23,019 - micro - MainProcess - INFO     Data converted to numpy array: [ 2  4  6  8 10 12 56 34] (utils.py:calculate_statistics:166)\n",
      "2024-05-27 13:43:23,020 - micro - MainProcess - INFO     Calculated median: 9.0 (utils.py:calculate_statistics:170)\n",
      "2024-05-27 13:43:23,025 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 12.0 (utils.py:calculate_statistics:174)\n",
      "2024-05-27 13:43:23,028 - micro - MainProcess - INFO     Calculated 95th percentile: 48.29999999999999 (utils.py:calculate_statistics:178)\n",
      "2024-05-27 13:43:23,032 - micro - MainProcess - INFO     Calculated 99th percentile: 54.459999999999994 (utils.py:calculate_statistics:182)\n",
      "2024-05-27 13:43:23,036 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 1.066649448896851 (utils.py:calculate_statistics:186)\n",
      "2024-05-27 13:43:23,039 - micro - MainProcess - INFO     Result: (9.0, 12.0, 48.29999999999999, 54.459999999999994, 1.066649448896851) (utils.py:calculate_statistics:189)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:43:23,048 - micro - MainProcess - INFO     Calculating statistics for data: [3, 4.5, 6.789, 1000, 456.678, 8769.98, 8967.9, 1000] (utils.py:calculate_statistics:158)\n",
      "2024-05-27 13:43:23,055 - micro - MainProcess - INFO     Data converted to numpy array: [3.00000e+00 4.50000e+00 6.78900e+00 1.00000e+03 4.56678e+02 8.76998e+03\n",
      " 8.96790e+03 1.00000e+03] (utils.py:calculate_statistics:166)\n",
      "2024-05-27 13:43:23,063 - micro - MainProcess - INFO     Calculated median: 728.3389999999999 (utils.py:calculate_statistics:170)\n",
      "2024-05-27 13:43:23,068 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 2936.27825 (utils.py:calculate_statistics:174)\n",
      "2024-05-27 13:43:23,072 - micro - MainProcess - INFO     Calculated 95th percentile: 8898.627999999999 (utils.py:calculate_statistics:178)\n",
      "2024-05-27 13:43:23,076 - micro - MainProcess - INFO     Calculated 99th percentile: 8954.0456 (utils.py:calculate_statistics:182)\n",
      "2024-05-27 13:43:23,081 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 1.4578329585969763 (utils.py:calculate_statistics:186)\n",
      "2024-05-27 13:43:23,085 - micro - MainProcess - INFO     Result: (728.3389999999999, 2936.27825, 8898.627999999999, 8954.0456, 1.4578329585969763) (utils.py:calculate_statistics:189)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:43:23,096 - micro - MainProcess - INFO     Calculating statistics for data: [2, 'none', 'none', 8, 10, 12, 56, 34] (utils.py:calculate_statistics:158)\n",
      "2024-05-27 13:43:23,101 - micro - MainProcess - INFO     Data converted to numpy array: ['2' 'none' 'none' '8' '10' '12' '56' '34'] (utils.py:calculate_statistics:166)\n",
      "2024-05-27 13:43:23,106 - micro - MainProcess - ERROR    An error occurred while calculating statistics: ufunc 'add' did not contain a loop with signature matching types (dtype('<U11'), dtype('<U11')) -> None (utils.py:calculate_statistics:193)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[33m                                                                                        [100%]\u001b[0m\n",
      "\u001b[33m======================================== warnings summary =========================================\u001b[0m\n",
      "..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\_pytest\\config\\__init__.py:1285\n",
      "  c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\_pytest\\config\\__init__.py:1285: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m\u001b[32m5 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 0.20s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.mark.parametrize(\"data, expected\", [\n",
    "    ([], (None, None, None, None, None)),\n",
    "    ([2]*5, (2.0, 0.0, 2.0, 2.0, 0.0)),\n",
    "    ([2,4,6,8,10,12,56,34],(9.0, 12.0, 48.29999999999999, 54.459999999999994, 1.066649448896851)),\n",
    "    ([3,4.5,6.789,1000,456.678,8769.98,8967.90,1000], (728.3389999999999, 2936.27825, 8898.627999999999, 8954.0456, 1.4578329585969763)),\n",
    "    ([2,\"none\",\"none\",8,10,12,56,34], (None, None, None, None, None)),\n",
    "])\n",
    "def test_calculate_statistics(data, expected):\n",
    "    \"\"\"\n",
    "    Test the calculate_statistics function with various types of data.\n",
    "    Each test case is designed to verify the function's capability to handle different data distributions,\n",
    "    including empty lists, repeated values, negative numbers, mixed data types, very small or very large numbers,\n",
    "    and lists with outliers. This ensures robust performance across a wide range of real-world scenarios.\n",
    "    \"\"\"\n",
    "    assert calculate_statistics(data) == expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Optional, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {\n",
    "    \"times_succesful\": [2,None,5,7,None],\n",
    "    \"times_unsucessfull\": [None,3,5,8,10],\n",
    "    \"regions\": [\"eastus\"],\n",
    "    \"number_of_iterations\": 6, \n",
    "    \"completion_tokens\": [45,67,89,97,56,78],\n",
    "    \"prompt_tokens\": [3,6,7,8,7,8,34,3456],\n",
    "    \"errors\": {\"count\": 3, \"codes\": [429,500,502,429,429]},\n",
    "    \"best_run\": {\n",
    "        \"time\": float(\"inf\"),\n",
    "        \"completion_tokens\": 0,\n",
    "        \"prompt_tokens\": 0,\n",
    "    },\n",
    "    \"worst_run\": {\n",
    "        \"time\": float(\"-inf\"),\n",
    "        \"completion_tokens\": 0,\n",
    "        \"prompt_tokens\": 0,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_statistics(data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate and return the statistical metrics for test results.\n",
    "    \n",
    "        :param data: Test data collected.\n",
    "        :return: Dictionary of calculated statistical metrics.\n",
    "        \"\"\"\n",
    "        total_requests = data[\"number_of_iterations\"]\n",
    "        times = list(filter(None, data.get(\"times_succesful\", [])))\n",
    "        completion_tokens = list(filter(None, data.get(\"completion_tokens\", [])))\n",
    "        prompt_tokens = list(filter(None, data.get(\"prompt_tokens\", [])))\n",
    "        error_count = data[\"errors\"][\"count\"]\n",
    "        error_codes = data[\"errors\"][\"codes\"]\n",
    "        error_distribution = {str(code): error_codes.count(code) for code in set(error_codes)}\n",
    "        count_throw = error_distribution.get('429', 0)\n",
    "        successful_runs = len(data['times_succesful'])\n",
    "        unsuccessful_runs = len(data['times_unsucessfull'])\n",
    "    \n",
    "        stats = {\n",
    "            \"median_time\": None,\n",
    "            \"regions\": list(set(data.get(\"regions\", []))),\n",
    "            \"iqr_time\": None,\n",
    "            \"percentile_95_time\": None,\n",
    "            \"percentile_99_time\": None,\n",
    "            \"cv_time\": None,\n",
    "            \"median_completion_tokens\": None,\n",
    "            \"iqr_completion_tokens\": None,\n",
    "            \"percentile_95_completion_tokens\": None,\n",
    "            \"percentile_99_completion_tokens\": None,\n",
    "            \"cv_completion_tokens\": None,\n",
    "            \"median_prompt_tokens\": None,\n",
    "            \"iqr_prompt_tokens\": None,\n",
    "            \"percentile_95_prompt_tokens\": None,\n",
    "            \"percentile_99_prompt_tokens\": None,\n",
    "            \"cv_prompt_tokens\": None,\n",
    "            \"error_rate\": error_count / total_requests if total_requests > 0 else 0,\n",
    "            \"number_of_iterations\": total_requests,\n",
    "            \"throttle_count\": count_throw,\n",
    "            \"throttle_rate\": count_throw / total_requests if total_requests > 0 else 0,\n",
    "            \"errors_types\": data.get(\"errors\", {}).get(\"codes\", []),\n",
    "            \"successful_runs\": successful_runs,\n",
    "            \"unsuccessful_runs\": unsuccessful_runs\n",
    "        }\n",
    "    \n",
    "        if times:\n",
    "            stats.update(zip(\n",
    "                [\"median_time\", \"iqr_time\", \"percentile_95_time\", \"percentile_99_time\", \"cv_time\"],\n",
    "                calculate_statistics(times)\n",
    "            ))\n",
    "    \n",
    "        if completion_tokens:\n",
    "            stats.update(zip(\n",
    "                [\"median_completion_tokens\", \"iqr_completion_tokens\", \"percentile_95_completion_tokens\", \"percentile_99_completion_tokens\", \"cv_completion_tokens\"],\n",
    "                calculate_statistics(completion_tokens)\n",
    "            ))\n",
    "    \n",
    "        if prompt_tokens:\n",
    "            stats.update(zip(\n",
    "                [\"median_prompt_tokens\", \"iqr_prompt_tokens\", \"percentile_95_prompt_tokens\", \"percentile_99_prompt_tokens\", \"cv_prompt_tokens\"],\n",
    "                calculate_statistics(prompt_tokens)\n",
    "            ))\n",
    "    \n",
    "        # Optional: Add best_run and worst_run if they're defined and valid\n",
    "        stats[\"best_run\"] = data.get(\"best_run\", {})\n",
    "        stats[\"worst_run\"] = data.get(\"worst_run\", {})\n",
    "    \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:00:05,167 - micro - MainProcess - INFO     Calculating statistics for data: [2, 5, 7] (utils.py:calculate_statistics:158)\n",
      "2024-05-27 13:00:05,167 - micro - MainProcess - INFO     Data converted to numpy array: [2 5 7] (utils.py:calculate_statistics:166)\n",
      "2024-05-27 13:00:05,172 - micro - MainProcess - INFO     Calculated median: 5.0 (utils.py:calculate_statistics:170)\n",
      "2024-05-27 13:00:05,181 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 2.5 (utils.py:calculate_statistics:174)\n",
      "2024-05-27 13:00:05,181 - micro - MainProcess - INFO     Calculated 95th percentile: 6.8 (utils.py:calculate_statistics:178)\n",
      "2024-05-27 13:00:05,188 - micro - MainProcess - INFO     Calculated 99th percentile: 6.96 (utils.py:calculate_statistics:182)\n",
      "2024-05-27 13:00:05,193 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.44031528592635544 (utils.py:calculate_statistics:186)\n",
      "2024-05-27 13:00:05,195 - micro - MainProcess - INFO     Result: (5.0, 2.5, 6.8, 6.96, 0.44031528592635544) (utils.py:calculate_statistics:189)\n",
      "2024-05-27 13:00:05,197 - micro - MainProcess - INFO     Calculating statistics for data: [45, 67, 89, 97, 56, 78] (utils.py:calculate_statistics:158)\n",
      "2024-05-27 13:00:05,200 - micro - MainProcess - INFO     Data converted to numpy array: [45 67 89 97 56 78] (utils.py:calculate_statistics:166)\n",
      "2024-05-27 13:00:05,202 - micro - MainProcess - INFO     Calculated median: 72.5 (utils.py:calculate_statistics:170)\n",
      "2024-05-27 13:00:05,205 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 27.5 (utils.py:calculate_statistics:174)\n",
      "2024-05-27 13:00:05,208 - micro - MainProcess - INFO     Calculated 95th percentile: 95.0 (utils.py:calculate_statistics:178)\n",
      "2024-05-27 13:00:05,210 - micro - MainProcess - INFO     Calculated 99th percentile: 96.6 (utils.py:calculate_statistics:182)\n",
      "2024-05-27 13:00:05,213 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.25102669836529556 (utils.py:calculate_statistics:186)\n",
      "2024-05-27 13:00:05,213 - micro - MainProcess - INFO     Result: (72.5, 27.5, 95.0, 96.6, 0.25102669836529556) (utils.py:calculate_statistics:189)\n",
      "2024-05-27 13:00:05,213 - micro - MainProcess - INFO     Calculating statistics for data: [3, 6, 7, 8, 7, 8, 34, 3456] (utils.py:calculate_statistics:158)\n",
      "2024-05-27 13:00:05,221 - micro - MainProcess - INFO     Data converted to numpy array: [   3    6    7    8    7    8   34 3456] (utils.py:calculate_statistics:166)\n",
      "2024-05-27 13:00:05,221 - micro - MainProcess - INFO     Calculated median: 7.5 (utils.py:calculate_statistics:170)\n",
      "2024-05-27 13:00:05,227 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 7.75 (utils.py:calculate_statistics:174)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:00:05,229 - micro - MainProcess - INFO     Calculated 95th percentile: 2258.2999999999984 (utils.py:calculate_statistics:178)\n",
      "2024-05-27 13:00:05,237 - micro - MainProcess - INFO     Calculated 99th percentile: 3216.459999999999 (utils.py:calculate_statistics:182)\n",
      "2024-05-27 13:00:05,245 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 2.5832862634989806 (utils.py:calculate_statistics:186)\n",
      "2024-05-27 13:00:05,246 - micro - MainProcess - INFO     Result: (7.5, 7.75, 2258.2999999999984, 3216.459999999999, 2.5832862634989806) (utils.py:calculate_statistics:189)\n"
     ]
    }
   ],
   "source": [
    "result = _calculate_statistics(data=data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.mark.parametrize(\"data, expected\", [\n",
    "    ([], (None, None, None, None, None)),\n",
    "    ([2]*5, (2.0, 0.0, 2.0, 2.0, 0.0)),\n",
    "    ([2,4,6,8,10,12,56,34],(9.0, 12.0, 48.29999999999999, 54.459999999999994, 1.066649448896851)),\n",
    "    ([3,4.5,6.789,1000,456.678,8769.98,8967.90,1000], (728.3389999999999, 2936.27825, 8898.627999999999, 8954.0456, 1.4578329585969763)),\n",
    "    ([2,\"none\",\"none\",8,10,12,56,34], (None, None, None, None, None)),\n",
    "])\n",
    "def test_calculate_statistics(data, expected):\n",
    "    \"\"\"\n",
    "    Test the calculate_statistics function with various types of data.\n",
    "    Each test case is designed to verify the function's capability to handle different data distributions,\n",
    "    including empty lists, repeated values, negative numbers, mixed data types, very small or very large numbers,\n",
    "    and lists with outliers. This ensures robust performance across a wide range of real-world scenarios.\n",
    "    \"\"\"\n",
    "    assert calculate_statistics(data) == expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:55:04,717 - micro - MainProcess - INFO     Calculating statistics for data: [2, 5, 7] (utils.py:calculate_statistics:158)\n",
      "2024-05-27 13:55:04,723 - micro - MainProcess - INFO     Data converted to numpy array: [2 5 7] (utils.py:calculate_statistics:166)\n",
      "2024-05-27 13:55:04,726 - micro - MainProcess - INFO     Calculated median: 5.0 (utils.py:calculate_statistics:170)\n",
      "2024-05-27 13:55:04,726 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 2.5 (utils.py:calculate_statistics:174)\n",
      "2024-05-27 13:55:04,734 - micro - MainProcess - INFO     Calculated 95th percentile: 6.8 (utils.py:calculate_statistics:178)\n",
      "2024-05-27 13:55:04,734 - micro - MainProcess - INFO     Calculated 99th percentile: 6.96 (utils.py:calculate_statistics:182)\n",
      "2024-05-27 13:55:04,741 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.44031528592635544 (utils.py:calculate_statistics:186)\n",
      "2024-05-27 13:55:04,741 - micro - MainProcess - INFO     Result: (5.0, 2.5, 6.8, 6.96, 0.44031528592635544) (utils.py:calculate_statistics:189)\n",
      "2024-05-27 13:55:04,741 - micro - MainProcess - INFO     Calculating statistics for data: [45, 67, 89, 97, 56, 78] (utils.py:calculate_statistics:158)\n",
      "2024-05-27 13:55:04,749 - micro - MainProcess - INFO     Data converted to numpy array: [45 67 89 97 56 78] (utils.py:calculate_statistics:166)\n",
      "2024-05-27 13:55:04,749 - micro - MainProcess - INFO     Calculated median: 72.5 (utils.py:calculate_statistics:170)\n",
      "2024-05-27 13:55:04,756 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 27.5 (utils.py:calculate_statistics:174)\n",
      "2024-05-27 13:55:04,759 - micro - MainProcess - INFO     Calculated 95th percentile: 95.0 (utils.py:calculate_statistics:178)\n",
      "2024-05-27 13:55:04,765 - micro - MainProcess - INFO     Calculated 99th percentile: 96.6 (utils.py:calculate_statistics:182)\n",
      "2024-05-27 13:55:04,770 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.25102669836529556 (utils.py:calculate_statistics:186)\n",
      "2024-05-27 13:55:04,774 - micro - MainProcess - INFO     Result: (72.5, 27.5, 95.0, 96.6, 0.25102669836529556) (utils.py:calculate_statistics:189)\n",
      "2024-05-27 13:55:04,777 - micro - MainProcess - INFO     Calculating statistics for data: [3, 6, 7, 8, 7, 8, 34, 3456] (utils.py:calculate_statistics:158)\n",
      "2024-05-27 13:55:04,777 - micro - MainProcess - INFO     Data converted to numpy array: [   3    6    7    8    7    8   34 3456] (utils.py:calculate_statistics:166)\n",
      "2024-05-27 13:55:04,784 - micro - MainProcess - INFO     Calculated median: 7.5 (utils.py:calculate_statistics:170)\n",
      "2024-05-27 13:55:04,785 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 7.75 (utils.py:calculate_statistics:174)\n",
      "2024-05-27 13:55:04,791 - micro - MainProcess - INFO     Calculated 95th percentile: 2258.2999999999984 (utils.py:calculate_statistics:178)\n",
      "2024-05-27 13:55:04,798 - micro - MainProcess - INFO     Calculated 99th percentile: 3216.459999999999 (utils.py:calculate_statistics:182)\n",
      "2024-05-27 13:55:04,802 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 2.5832862634989806 (utils.py:calculate_statistics:186)\n",
      "2024-05-27 13:55:04,805 - micro - MainProcess - INFO     Result: (7.5, 7.75, 2258.2999999999984, 3216.459999999999, 2.5832862634989806) (utils.py:calculate_statistics:189)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[33m                                                                                            [100%]\u001b[0m\n",
      "\u001b[33m======================================== warnings summary =========================================\u001b[0m\n",
      "..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\_pytest\\config\\__init__.py:1285\n",
      "  c:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\_pytest\\config\\__init__.py:1285: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m\u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 0.13s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "from math import inf\n",
    "\n",
    "@pytest.mark.parametrize(\"data, expected\", [\n",
    "    ({\n",
    "    \"times_succesful\": [2,None,5,7,None],\n",
    "    \"times_unsucessfull\": [None,3,5,8,10],\n",
    "    \"regions\": [\"eastus\"],\n",
    "    \"number_of_iterations\": 6, \n",
    "    \"completion_tokens\": [45,67,89,97,56,78],\n",
    "    \"prompt_tokens\": [3,6,7,8,7,8,34,3456],\n",
    "    \"errors\": {\"count\": 3, \"codes\": [429,500,502,429,429]},\n",
    "    \"best_run\": {\n",
    "        \"time\": inf,\n",
    "        \"completion_tokens\": 0,\n",
    "        \"prompt_tokens\": 0,\n",
    "    },\n",
    "    \"worst_run\": {\n",
    "        \"time\": -inf,\n",
    "        \"completion_tokens\": 0,\n",
    "        \"prompt_tokens\": 0,\n",
    "    }\n",
    "}, {'median_time': 5.0,\n",
    " 'regions': ['eastus'],\n",
    " 'iqr_time': 2.5,\n",
    " 'percentile_95_time': 6.8,\n",
    " 'percentile_99_time': 6.96,\n",
    " 'cv_time': 0.44031528592635544,\n",
    " 'median_completion_tokens': 72.5,\n",
    " 'iqr_completion_tokens': 27.5,\n",
    " 'percentile_95_completion_tokens': 95.0,\n",
    " 'percentile_99_completion_tokens': 96.6,\n",
    " 'cv_completion_tokens': 0.25102669836529556,\n",
    " 'median_prompt_tokens': 7.5,\n",
    " 'iqr_prompt_tokens': 7.75,\n",
    " 'percentile_95_prompt_tokens': 2258.2999999999984,\n",
    " 'percentile_99_prompt_tokens': 3216.459999999999,\n",
    " 'cv_prompt_tokens': 2.5832862634989806,\n",
    " 'error_rate': 0.5,\n",
    " 'number_of_iterations': 6,\n",
    " 'throttle_count': 3,\n",
    " 'throttle_rate': 0.5,\n",
    " 'errors_types': [429, 500, 502, 429, 429],\n",
    " 'successful_runs': 5,\n",
    " 'unsuccessful_runs': 5,\n",
    " 'best_run': {'time': inf, 'completion_tokens': 0, 'prompt_tokens': 0},\n",
    " 'worst_run': {'time': -inf, 'completion_tokens': 0, 'prompt_tokens': 0}}),\n",
    "])\n",
    "def test_calculate_statistics2(data, expected):\n",
    "    \"\"\"\n",
    "    Test the calculate_statistics function with various types of data.\n",
    "    Each test case is designed to verify the function's capability to handle different data distributions,\n",
    "    including empty lists, repeated values, negative numbers, mixed data types, very small or very large numbers,\n",
    "    and lists with outliers. This ensures robust performance across a wide range of real-world scenarios.\n",
    "    \"\"\"\n",
    "    assert _calculate_statistics(data) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'median_time': 5.0,\n",
       " 'regions': ['eastus'],\n",
       " 'iqr_time': 2.5,\n",
       " 'percentile_95_time': 6.8,\n",
       " 'percentile_99_time': 6.96,\n",
       " 'cv_time': 0.44031528592635544,\n",
       " 'median_completion_tokens': 72.5,\n",
       " 'iqr_completion_tokens': 27.5,\n",
       " 'percentile_95_completion_tokens': 95.0,\n",
       " 'percentile_99_completion_tokens': 96.6,\n",
       " 'cv_completion_tokens': 0.25102669836529556,\n",
       " 'median_prompt_tokens': 7.5,\n",
       " 'iqr_prompt_tokens': 7.75,\n",
       " 'percentile_95_prompt_tokens': 2258.2999999999984,\n",
       " 'percentile_99_prompt_tokens': 3216.459999999999,\n",
       " 'cv_prompt_tokens': 2.5832862634989806,\n",
       " 'error_rate': 0.5,\n",
       " 'number_of_iterations': 6,\n",
       " 'throttle_count': 3,\n",
       " 'throttle_rate': 0.5,\n",
       " 'errors_types': [429, 500, 502, 429, 429],\n",
       " 'successful_runs': 5,\n",
       " 'unsuccessful_runs': 5,\n",
       " 'best_run': {'time': inf, 'completion_tokens': 0, 'prompt_tokens': 0},\n",
       " 'worst_run': {'time': -inf, 'completion_tokens': 0, 'prompt_tokens': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ml_logging import get_logger\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Set up logger\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(\n",
    "    data: List[float],\n",
    ") -> Tuple[\n",
    "    Optional[float], Optional[float], Optional[float], Optional[float], Optional[float]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Calculate statistical measures for a list of numbers.\n",
    "\n",
    "    Parameters:\n",
    "    data (List[float]): Input list of numbers.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[Optional[float], Optional[float], Optional[float], Optional[float], Optional[float]]:\n",
    "    A tuple containing the following statistics, or None for each if the input list is empty or an error occurs:\n",
    "\n",
    "    - Median: Middle value in the sorted list. Less affected by outliers.\n",
    "    - Interquartile Range (IQR): Measures statistical dispersion. Difference between 75th and 25th percentiles.\n",
    "    - 95th Percentile: Value below which 95% of observations fall.\n",
    "    - 99th Percentile: Value below which 99% of observations fall.\n",
    "    - Coefficient of Variation (CV): Ratio of standard deviation to mean. Useful for comparing variability across data series.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Calculating statistics for data: {data}\")\n",
    "\n",
    "        if not data:\n",
    "            result = (None, None, None, None, None)\n",
    "            logger.info(f\"No data provided. Returning result: {result}\")\n",
    "            return result\n",
    "\n",
    "        data_array = np.array(data)\n",
    "        logger.info(f\"Data converted to numpy array: {data_array}\")\n",
    "\n",
    "        # Calculate the median\n",
    "        median = np.median(data_array)\n",
    "        logger.info(f\"Calculated median: {median}\")\n",
    "\n",
    "        # Calculate the IQR\n",
    "        iqr = stats.iqr(data_array)\n",
    "        logger.info(f\"Calculated interquartile range (IQR): {iqr}\")\n",
    "\n",
    "        # Calculate the 95th percentile\n",
    "        percentile_95 = np.percentile(data_array, 95)\n",
    "        logger.info(f\"Calculated 95th percentile: {percentile_95}\")\n",
    "\n",
    "        # Calculate the 99th percentile\n",
    "        percentile_99 = np.percentile(data_array, 99)\n",
    "        logger.info(f\"Calculated 99th percentile: {percentile_99}\")\n",
    "\n",
    "        # Calculate the coefficient of variation\n",
    "        cv = stats.variation(data_array)\n",
    "        logger.info(f\"Calculated coefficient of variation (CV): {cv}\")\n",
    "\n",
    "        result = (median, iqr, percentile_95, percentile_99, cv)\n",
    "        logger.info(f\"Result: {result}\")\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while calculating statistics: {e}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_1 = [3,4.5,6.789,1000,456.678,8769.98,8967.90,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 15:46:10,897 - micro - MainProcess - INFO     Calculating statistics for data: [3, 4.5, 6.789, 1000, 456.678, 8769.98, 8967.9, 1000] (1673067920.py:calculate_statistics:23)\n",
      "2024-05-23 15:46:10,898 - micro - MainProcess - INFO     Data converted to numpy array: [3.00000e+00 4.50000e+00 6.78900e+00 1.00000e+03 4.56678e+02 8.76998e+03\n",
      " 8.96790e+03 1.00000e+03] (1673067920.py:calculate_statistics:31)\n",
      "2024-05-23 15:46:10,898 - micro - MainProcess - INFO     Calculated median: 728.3389999999999 (1673067920.py:calculate_statistics:35)\n",
      "2024-05-23 15:46:10,905 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 2936.27825 (1673067920.py:calculate_statistics:39)\n",
      "2024-05-23 15:46:10,906 - micro - MainProcess - INFO     Calculated 95th percentile: 8898.627999999999 (1673067920.py:calculate_statistics:43)\n",
      "2024-05-23 15:46:10,906 - micro - MainProcess - INFO     Calculated 99th percentile: 8954.0456 (1673067920.py:calculate_statistics:47)\n",
      "2024-05-23 15:46:10,913 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 1.4578329585969763 (1673067920.py:calculate_statistics:51)\n",
      "2024-05-23 15:46:10,918 - micro - MainProcess - INFO     Result: (728.3389999999999, 2936.27825, 8898.627999999999, 8954.0456, 1.4578329585969763) (1673067920.py:calculate_statistics:54)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(728.3389999999999,\n",
       " 2936.27825,\n",
       " 8898.627999999999,\n",
       " 8954.0456,\n",
       " 1.4578329585969763)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_statistics(lista_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                       [100%]\u001b[0m\n",
      "============================================ FAILURES =============================================\n",
      "\u001b[31m\u001b[1m___________________________________ test_run_latency_benchmark ____________________________________\u001b[0m\n",
      "\n",
      "self = <Coroutine test_run_latency_benchmark>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mruntest\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m) -> \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.obj = wrap_in_sync(\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# https://github.com/pytest-dev/pytest-asyncio/issues/596\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.obj,  \u001b[90m# type: ignore[has-type]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[96msuper\u001b[39;49;00m().runtest()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:436: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\u001b[1m\u001b[31m..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:902: in inner\n",
      "    \u001b[0m_loop.run_until_complete(task)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\asyncio\\base_events.py\u001b[0m:623: in run_until_complete\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._check_running()\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "self = <_WindowsSelectorEventLoop running=True closed=False debug=False>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_check_running\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_running():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mRuntimeError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mThis event loop is already running\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: This event loop is already running\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\asyncio\\base_events.py\u001b[0m:583: RuntimeError\n",
      "\u001b[31m\u001b[1m________________________________ test_error_handling_in_api_calls _________________________________\u001b[0m\n",
      "\n",
      "self = <Coroutine test_error_handling_in_api_calls>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mruntest\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m) -> \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.obj = wrap_in_sync(\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# https://github.com/pytest-dev/pytest-asyncio/issues/596\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.obj,  \u001b[90m# type: ignore[has-type]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[96msuper\u001b[39;49;00m().runtest()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:436: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\u001b[1m\u001b[31m..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:902: in inner\n",
      "    \u001b[0m_loop.run_until_complete(task)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\asyncio\\base_events.py\u001b[0m:623: in run_until_complete\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._check_running()\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "self = <_WindowsSelectorEventLoop running=True closed=False debug=False>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_check_running\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_running():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mRuntimeError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mThis event loop is already running\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: This event loop is already running\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\asyncio\\base_events.py\u001b[0m:583: RuntimeError\n",
      "\u001b[31m\u001b[1m_________________________________ test_run_latency_benchmark_bulk _________________________________\u001b[0m\n",
      "\n",
      "self = <Coroutine test_run_latency_benchmark_bulk>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mruntest\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m) -> \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96mself\u001b[39;49;00m.obj = wrap_in_sync(\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[90m# https://github.com/pytest-dev/pytest-asyncio/issues/596\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m.obj,  \u001b[90m# type: ignore[has-type]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[96msuper\u001b[39;49;00m().runtest()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:436: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\u001b[1m\u001b[31m..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\site-packages\\pytest_asyncio\\plugin.py\u001b[0m:902: in inner\n",
      "    \u001b[0m_loop.run_until_complete(task)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\asyncio\\base_events.py\u001b[0m:623: in run_until_complete\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m._check_running()\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "self = <_WindowsSelectorEventLoop running=True closed=False debug=False>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_check_running\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_running():\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mraise\u001b[39;49;00m \u001b[96mRuntimeError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mThis event loop is already running\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: This event loop is already running\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m..\\..\\AppData\\Local\\anaconda3\\envs\\ptu-benchmarking\\lib\\asyncio\\base_events.py\u001b[0m:583: RuntimeError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info =====================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_642d87945d5148bebd69921f67fcf83a.py::\u001b[1mtest_run_latency_benchmark\u001b[0m - RuntimeError: This event loop is already running\n",
      "\u001b[31mFAILED\u001b[0m t_642d87945d5148bebd69921f67fcf83a.py::\u001b[1mtest_error_handling_in_api_calls\u001b[0m - RuntimeError: This event loop is already running\n",
      "\u001b[31mFAILED\u001b[0m t_642d87945d5148bebd69921f67fcf83a.py::\u001b[1mtest_run_latency_benchmark_bulk\u001b[0m - RuntimeError: This event loop is already running\n",
      "\u001b[31m\u001b[31m\u001b[1m3 failed\u001b[0m, \u001b[32m3 passed\u001b[0m\u001b[31m in 1.21s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "import pytest\n",
    "import asyncio\n",
    "from unittest.mock import AsyncMock, patch\n",
    "from src.performance.latencytest import AzureOpenAIBenchmarkLatency\n",
    "from src.performance.aoaihelpers.utils import calculate_statistics\n",
    "\n",
    "@pytest.mark.parametrize(\"data, expected\", [\n",
    "    ([], (None, None, None, None, None)),\n",
    "    ([2]*5, (2.0, 0.0, 2.0, 2.0, 0.0)),\n",
    "    ([2, 4, 6, 8, 10, 12, 56, 34], (9.0, 12.0, 48.29999999999999, 54.459999999999994, 1.066649448896851))\n",
    "])\n",
    "def test_calculate_statistics(data, expected):\n",
    "    \"\"\"\n",
    "    Test the calculate_statistics function with various types of data.\n",
    "    Each test case is designed to verify the function's capability to handle different data distributions,\n",
    "    including empty lists, repeated values, negative numbers, mixed data types, very small or very large numbers,\n",
    "    and lists with outliers. This ensures robust performance across a wide range of real-world scenarios.\n",
    "    \"\"\"\n",
    "    assert calculate_statistics(data) == expected\n",
    "\n",
    "# Mock for dependencies\n",
    "@pytest.fixture\n",
    "def benchmark():\n",
    "    with patch('src.performance.latencytest.AzureOpenAIBenchmarkLatency', autospec=True) as mock_class:\n",
    "        instance = mock_class.return_value\n",
    "        instance.make_call = AsyncMock()\n",
    "        return instance  # Return the mock instance, not the class\n",
    "\n",
    "# Test the main functionality of making asynchronous API calls\n",
    "@pytest.mark.asyncio\n",
    "async def test_run_latency_benchmark(benchmark):\n",
    "    deployment_names = [\"model1\", \"model2\"]\n",
    "    max_tokens_list = [10, 20]\n",
    "    iterations = 2\n",
    "\n",
    "    # Setup the async method to simulate API call\n",
    "    benchmark.make_call = AsyncMock()  # Mock the method directly on the instance\n",
    "\n",
    "    await benchmark.run_latency_benchmark(deployment_names, max_tokens_list, iterations)\n",
    "\n",
    "    assert benchmark.make_call.call_count == len(deployment_names) * len(max_tokens_list) * iterations\n",
    "\n",
    "# Ensure proper handling of errors during API calls\n",
    "@pytest.mark.asyncio\n",
    "async def test_error_handling_in_api_calls(benchmark):\n",
    "    benchmark.make_call.side_effect = Exception(\"API call failed\")\n",
    "    try:\n",
    "        await benchmark.make_call(\"model1\", 10)\n",
    "    except Exception as e:\n",
    "        assert str(e) == \"API call failed\"\n",
    "\n",
    "# Run concurrency tests\n",
    "@pytest.mark.asyncio\n",
    "async def test_run_latency_benchmark_bulk(benchmark):\n",
    "    deployment_names = [\"model1\", \"model2\"]\n",
    "    max_tokens_list = [10, 20]\n",
    "    iterations = 1\n",
    "\n",
    "    # Mock asyncio.gather to observe its call\n",
    "    with patch('asyncio.gather', new_callable=AsyncMock) as mock_gather:\n",
    "        await benchmark.run_latency_benchmark_bulk(deployment_names, max_tokens_list)\n",
    "\n",
    "        # Check that asyncio.gather was called correctly\n",
    "        assert mock_gather.call_count == 1\n",
    "        assert len(mock_gather.call_args[0][0]) == len(deployment_names) * len(max_tokens_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptu-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
